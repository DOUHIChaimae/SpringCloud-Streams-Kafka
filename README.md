# Event Driven Architecture

Cette activit√© se focalise sur la mise en place de l'Event Driven Architecture en utilisant Apache Kafka et Spring Cloud
Streams. L'EDA est un mod√®le de conception qui permet de cr√©er des syst√®mes r√©actifs en temps r√©el en r√©agissant aux
√©v√©nements.

Nous aborderons les √©tapes cl√©s, de l'installation de Kafka √† l'utilisation de Docker pour simplifier le d√©ploiement. De
plus, nous d√©velopperons divers services Kafka, tels qu'un producteur, un consommateur, un fournisseur et un service de
traitement en temps r√©el.

L'objectif final est la cr√©ation d'une application web pour visualiser en temps r√©el les r√©sultats du traitement des
donn√©es.

## Table de mati√®re

- [D√©marrer Zookeeper](#D√©marrer Zookeeper)
- [D√©marrer Kafka-server](#D√©marrer Kafka-server)
- [Tester avec Kafka-console-producer et kafka-console-consumer](#Tester avec Kafka-console-producer et kafka-console-consumer)
- [Avec Docker](#Avec Docker)
- [Avec KAFKA & Spring Cloud Streams](#AvecKAFKA&StpringCloudStreams)
  
![Architecture](.jpg)
## D√©marrer Zookeeper

Apr√®s le t√©l√©chargement du Kafka on va d√©marrer zookeeper qui est utilis√© comme composant de base pour la gestion des brokers Kafka

Pour le d√©marrer, on va ex√©cuter la commande suivante :

```shell
start bin\windows\zookeeper-server-start.bat config/zookeeper.properties
```
![zookeeper](.jpg)

Zookeeper, maintenant, est d√©marr√© sur le port 2181 :

## D√©marrer Kafka-server

On va d√©marrer Kafka-server √† l‚Äôaide de cette commande :
```shell
start bin\windows\kafka-server-start.bat config/server.properties
```
Apr√®s l'ex√©cution de la commande, Kafka va √©couter sur l'adresse IP 0.0.0.0 et sur le port 9092.

## Tester avec Kafka-console-producer et kafka-console-consumer
* kafka-console-consumer

L'utilitaire kafka-console-consumer est utilis√© pour consommer des messages √† partir d'un topic Kafka √† partir de la
ligne de commande.

On va d√©marrer notre consommateur de console Kafka sur un syst√®me Windows, le configurer pour se connecter √† un broker
Kafka sur localhost:9092 et lui indique de lire les messages du topic R1 en √©x√©cutant la commande : 
```shell
start bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic R1
```

* kafka-console-producer

L'utilitaire kafka-console-producer est utilis√© pour produire (envoyer) des messages vers un topic Kafka √† partir de la
ligne de commande.

Comme la commande montre, ce producteur doit se connecter au brokers Kafka fonctionnant sur localhost sur le port 9092
pour envoyer des messages au topic nomm√© R1
<br/><br/>
```shell
start bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic R1
```

Alors le consommateur attend les messages envoy√©s vers le topic <br/><br/>
![image](.jpg)

## Avec Docker

* Cr√©er le fichier docker-compose.yml

![image](.jpg)

* D√©marrer les conteneurs docker : zookeeper et kafka-broker

![image](.jpg)

* Tester avec Kafka-console-producer et kafka-console-consumer

![image](.jpg)

![image](.jpg)

## Avec KAFKA & Stpring Cloud Streams

* Un Service Producer KAFKA via un RestController

  ```java
  @GetMapping("/publish/{topic}/{name}")
  public PageEvent publish(@PathVariable String topic, @PathVariable String name) {
  PageEvent pageEvent = new PageEvent(name, Math.random() > 0.5 ? "U1" : "U2", new Date(), new Random().nextInt(9000));
  streamBridge.send(topic, pageEvent);
  return pageEvent;
  }
  
* Un Service Consumer KAFKA
  <br><br>
  Le r√¥le de ce service est de consommer des √©v√©nements de type PageEvent et d'afficher ces √©v√©nements dans la console.
  Il sert principalement √† observer les √©v√©nements re√ßus dans le syst√®me et √† fournir une visibilit√© sur les donn√©es qui
  transitent par le flux de donn√©es Kafka.

  ```java
    @Bean
    public Consumer<PageEvent> pageEventConsumer() {
        return (input) -> {
            System.out.println("*************************");
            System.out.println(input.toString());
            System.out.println("*************************");
        };
    }
  
* Un Service Supplier KAFKA
  <br><br>
  Nous avons cr√©√© un Supplier de PageEvent qui est une interface fonctionnelle de Java qui ne prend aucun argument et
  renvoie un r√©sultat. Il g√©n√®re al√©atoirement des objets PageEvent √† chaque appel.

  ```java
    @Bean
    public Supplier<PageEvent> pageEventSupplier() {
        return () -> new PageEvent(
                Math.random() < 0.5 ? "P1" : "P2",
                Math.random() > 0.5 ? "U1" : "U2",
                new Date(),
                new Random().nextInt(9000));
    }
* Un Service Function KAFKA
  ```java
    @Bean
    public Function<PageEvent, PageEvent> pageEventFunction() {
        return (input) -> {
            input.setName("Page event");
            input.setUser("user");
            return input;
        };
    }
  
* Un Service de Data Analytics Real Time Stream Processing avec Kafka Streams
  Un service de Data Analytics en temps r√©el bas√© sur Kafka Streams est un syst√®me qui permet de traiter, analyser et
  transformer des donn√©es en continu √† mesure qu'elles sont g√©n√©r√©es. Kafka Streams est une biblioth√®que open-source
  pour le traitement des flux de donn√©es en temps r√©el, con√ßue pour √™tre int√©gr√©e avec Apache Kafka, une plateforme de
  streaming de donn√©es.

  ```java
    @Bean
    public Function<KStream<String, PageEvent>, KStream<String, Long>> kStreamFunction() {
        return (input) -> input
                .filter((k, v) -> v.getDuration() > 100)
                .map((k, v) -> new KeyValue<>(v.getName(), 0L))
                .groupBy((k, v) -> k, Grouped.with(Serdes.String(), Serdes.Long()))
                .windowedBy(TimeWindows.of(Duration.ofSeconds(5)))
                .count(Materialized.as("page-count"))
                .toStream()
                .map((k, v) -> new KeyValue<>("=>" + k.window().startTime() + k.window().endTime() + k.key(), v));
    }


On va ajouter ces propri√©t√©s :

  ```properties
  spring.cloud.stream.bindings.kStreamFunction-in-0.destination=R2
  spring.cloud.stream.bindings.kStreamFunction-out-0.destination=R4
  spring.cloud.stream.kafka.streams.binder.configuration.commit.interval.ms=1000
  ```
* On va ex√©cuter la commande suivante pour d√©marrer un consommateur Kafka en mode console, le configure pour se connecter
√† un serveur Kafka local, lire les messages du sujet "R4", et afficher √† la fois les cl√©s et les valeurs des messages,
avec la configuration sp√©cifi√©e pour les d√©s√©rialiseurs de cl√© et de valeur.

  ```shell
  start bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic R4 --property print.value=true
  --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property
  value.deserializer=org.apache.kafka.common.serialization.LongDeserializer
  ```
* Une application Web qui permet d'afficher les r√©sultats du Stream Data Analytics en temps r√©el
  On va exposer (/analytics) pour fournir un flux continu de donn√©es d'analyse en temps r√©el au format Server-Sent
  Events

  ```java
    @GetMapping(value = "/analytics", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<Map<String, Long>> analytics() {
        return Flux.interval(Duration.ofSeconds(1))
                .map(seq -> {
                    Map<String, Long> stringLongMap = new HashMap<>();
                    ReadOnlyWindowStore<String, Long> windowStore = interactiveQueryService.getQueryableStore("page-count",
                            QueryableStoreTypes.windowStore());
                    Instant now = Instant.now();
                    Instant from = now.minusMillis(5000);
                    KeyValueIterator<Windowed<String>, Long> fetchAll = windowStore.fetchAll(from, now);
                    while (fetchAll.hasNext()) {
                        KeyValue<Windowed<String>, Long> next = fetchAll.next();
                        stringLongMap.put(next.key.key(), next.value);
                    }
                    return stringLongMap;
                }).share();
    }
  
On va utiliser Smoothie une biblioth√®que JavaScript populaire pour la cr√©ation de graphiques en temps r√©el et
d'animations de mani√®re fluide dans le navigateur web

```javascript

<script>
    var index = -1;
    randomColor = function () {
        ++index;
        if (index >= colors.length) index = 0;
        return colors[index];
    }
    var pages = ["P1", "P2"];
    var colors = [
        {sroke: 'rgba(0, 255, 0, 1)', fill: 'rgba(0, 255, 0, 0.2)'},
        {
            sroke: 'rgba(255, 0, 0, 1)', fill: 'rgba(255, 0, 0, 0.2)'
        }];
    var courbe = [];
    var smoothieChart = new SmoothieChart({tooltip: true});
    smoothieChart.streamTo(document.getElementById("chart2"), 500);
    pages.forEach(function (v) {
        courbe[v] = new TimeSeries();
        col = randomColor();
        smoothieChart.addTimeSeries(courbe[v], {strokeStyle: col.sroke, fillStyle: col.fill, lineWidth: 2});
    });
    var stockEventSource = new EventSource("/analytics");
    stockEventSource.addEventListener("message", function (event) {
        pages.forEach(function (v) {
            val = JSON.parse(event.data)[v];
            courbe[v].append(new Date().getTime(), val);
        });
    });
</script>
```
Nous avons ajou√© ce script pour cr√©er un graphique en temps r√©el en utilisant la biblioth√®que SmoothieChart. 

Il r√©cup√®re des donn√©es d'√©v√©nements provenant d'une source externe via EventSource et les affiche graphiquement en mettant √† jour le graphique en continu. Cela permet de visualiser en temps r√©el les donn√©es relatives √† des pages sp√©cifiques (P1 et P2) dans des couleurs diff√©rentes.
## üîó About me :

[![linkedin](https://img.shields.io/badge/linkedin-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/chaimae-douhi/)
